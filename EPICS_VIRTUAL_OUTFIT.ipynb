{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ACGPN_and_SieveNet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7WJtP2PfBcPN",
        "veedN5N8BYOl"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faijan-khan/brainbuddy/blob/main/EPICS_VIRTUAL_OUTFIT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CCSQIBMAYYZ"
      },
      "source": [
        "For inferencing ACGPN and SieveNet!\n",
        "\n",
        "ACGPN repo: https://github.com/switchablenorms/DeepFashion_Try_On\n",
        "\n",
        "SieveNet repo: https://github.com/levindabhi/SieveNet\n",
        "\n",
        "This notebook is hard coded for inferencing one image at a time.\n",
        "\n",
        "Notebook by [Levin Dabhi](https://levindabhi.github.io/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WJtP2PfBcPN"
      },
      "source": [
        "# ACGPN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVm5QFBMDBbT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f05ddd15-e0f0-4fc0-847d-539a2268f0d7"
      },
      "source": [
        "!git clone https://github.com/levindabhi/ACGPN.git\n",
        "%cd ACGPN"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ACGPN'...\n",
            "remote: Enumerating objects: 161, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 161 (delta 20), reused 18 (delta 18), pack-reused 139 (from 1)\u001b[K\n",
            "Receiving objects: 100% (161/161), 302.85 KiB | 1.17 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n",
            "/content/ACGPN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnk7syY0rPKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92cec087-86cc-43ee-c400-142f5fc0f947"
      },
      "source": [
        "!pip install ninja"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/422.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m317.4/422.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQwI--uhoH6R"
      },
      "source": [
        "import gdown\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import IPython\n",
        "import gdown\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from predict_pose import generate_pose_keypoints"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwQ5zfh5oMe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6be30a2d-8f99-4ba6-ae38-886701e7f106"
      },
      "source": [
        "'''\n",
        "gdown.download('https://drive.google.com/uc?id=1tE7hcVFm8Td8kRh5iYRBSDFdvZIkbUIR', 'Data_preprocessing/data.zip', quiet=False)\n",
        "%cd Data_preprocessing\n",
        "!unzip data\n",
        "%cd ..\n",
        "'''"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ngdown.download('https://drive.google.com/uc?id=1tE7hcVFm8Td8kRh5iYRBSDFdvZIkbUIR', 'Data_preprocessing/data.zip', quiet=False)\\n%cd Data_preprocessing\\n!unzip data\\n%cd ..\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86Ll7LIW1kGw"
      },
      "source": [
        "!mkdir Data_preprocessing/test_color\n",
        "!mkdir Data_preprocessing/test_colormask\n",
        "!mkdir Data_preprocessing/test_edge\n",
        "!mkdir Data_preprocessing/test_img\n",
        "!mkdir Data_preprocessing/test_label\n",
        "!mkdir Data_preprocessing/test_mask\n",
        "!mkdir Data_preprocessing/test_pose\n",
        "!mkdir inputs\n",
        "!mkdir inputs/img\n",
        "!mkdir inputs/cloth"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8hYM6XqCnxC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57282b67-18a6-4693-a957-035a60038783"
      },
      "source": [
        "%cd pose\n",
        "!gdown --id 1hOHMFHEjhoJuLEQY0Ndurn5hfiA9mwko\n",
        "%cd .."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ACGPN/pose\n",
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Failed to retrieve file url:\n",
            "\n",
            "\tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses.\n",
            "\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=1hOHMFHEjhoJuLEQY0Ndurn5hfiA9mwko\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n",
            "/content/ACGPN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbGDB31KrKHV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e25e73c-6e08-4b8a-9955-06b9208a2d81"
      },
      "source": [
        "!git clone https://github.com/levindabhi/Self-Correction-Human-Parsing-for-ACGPN.git\n",
        "!git clone https://github.com/levindabhi/U-2-Net.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Self-Correction-Human-Parsing-for-ACGPN'...\n",
            "remote: Enumerating objects: 769, done.\u001b[K\n",
            "remote: Counting objects: 100% (203/203), done.\u001b[K\n",
            "remote: Compressing objects: 100% (109/109), done.\u001b[K\n",
            "remote: Total 769 (delta 103), reused 94 (delta 94), pack-reused 566 (from 1)\u001b[K\n",
            "Receiving objects: 100% (769/769), 3.89 MiB | 8.02 MiB/s, done.\n",
            "Resolving deltas: 100% (184/184), done.\n",
            "Cloning into 'U-2-Net'...\n",
            "remote: Enumerating objects: 822, done.\u001b[K\n",
            "remote: Counting objects: 100% (334/334), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 822 (delta 312), reused 299 (delta 299), pack-reused 488 (from 1)\u001b[K\n",
            "Receiving objects: 100% (822/822), 30.71 MiB | 24.58 MiB/s, done.\n",
            "Resolving deltas: 100% (391/391), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hSJI347rZtQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ef6125f5-cbb7-4671-d1f8-cbde256e1943"
      },
      "source": [
        "#for segmentation mask generation\n",
        "url = 'https://drive.google.com/uc?id=1k4dllHpu0bdx38J7H28rVVLpU-kOHmnH'\n",
        "output = 'lip_final.pth'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1k4dllHpu0bdx38J7H28rVVLpU-kOHmnH\n",
            "From (redirected): https://drive.google.com/uc?id=1k4dllHpu0bdx38J7H28rVVLpU-kOHmnH&confirm=t&uuid=c01032a6-dfec-493e-ab7d-d391091b949d\n",
            "To: /content/ACGPN/lip_final.pth\n",
            "100%|██████████| 267M/267M [00:01<00:00, 153MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lip_final.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzy8poZT6pcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4201cc68-103e-4b86-8416-2e5ca532910c"
      },
      "source": [
        "%cd U-2-Net\n",
        "!mkdir saved_models\n",
        "!mkdir saved_models/u2net\n",
        "!mkdir saved_models/u2netp\n",
        "!gdown --id 1rbSTGKAE-MTxBYHd-51l2hMOQPT_7EPy -O saved_models/u2netp/u2netp.pth\n",
        "!gdown --id 1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ -O saved_models/u2net/u2net.pth\n",
        "import u2net_load\n",
        "import u2net_run\n",
        "u2net = u2net_load.model(model_name = 'u2netp')\n",
        "%cd .."
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ACGPN/U-2-Net\n",
            "mkdir: cannot create directory ‘saved_models’: File exists\n",
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rbSTGKAE-MTxBYHd-51l2hMOQPT_7EPy\n",
            "To: /content/ACGPN/U-2-Net/saved_models/u2netp/u2netp.pth\n",
            "100% 4.68M/4.68M [00:00<00:00, 32.7MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ\n",
            "From (redirected): https://drive.google.com/uc?id=1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ&confirm=t&uuid=807da0fe-ebe9-4a87-8555-2e333263e6ca\n",
            "To: /content/ACGPN/U-2-Net/saved_models/u2net/u2net.pth\n",
            "100% 176M/176M [00:01<00:00, 148MB/s]\n",
            "...load U2NEP---4.7 MB\n",
            "/content/ACGPN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1VknOqswSTW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78748b14-3bea-43ee-fed1-f9d23d69cebd"
      },
      "source": [
        "!mkdir checkpoints\n",
        "gdown.download('https://drive.google.com/uc?id=1UWT6esQIU_d4tUm8cjxDKMhB8joQbrFx',output='checkpoints/ACGPN_checkpoints.zip', quiet=False)\n",
        "%cd checkpoints\n",
        "!unzip ACGPN_checkpoints\n",
        "%cd .."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1UWT6esQIU_d4tUm8cjxDKMhB8joQbrFx\n",
            "From (redirected): https://drive.google.com/uc?id=1UWT6esQIU_d4tUm8cjxDKMhB8joQbrFx&confirm=t&uuid=c343ec9f-d6f4-43c3-bd67-7dddc23a2ccc\n",
            "To: /content/ACGPN/checkpoints/ACGPN_checkpoints.zip\n",
            "100%|██████████| 524M/524M [00:03<00:00, 155MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ACGPN/checkpoints\n",
            "Archive:  ACGPN_checkpoints.zip\n",
            "   creating: label2city/\n",
            "  inflating: label2city/latest_net_G.pth  \n",
            "  inflating: label2city/latest_net_G1.pth  \n",
            "  inflating: label2city/latest_net_G2.pth  \n",
            "  inflating: label2city/latest_net_U.pth  \n",
            "  inflating: label2city/opt.txt      \n",
            "/content/ACGPN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O pose/pose_iter_440000.caffemodel \\\n",
        "  https://github.com/nik1806/Human-Pose-Detection/raw/refs/heads/master/model/coco/pose_iter_440000.caffemodel\n"
      ],
      "metadata": {
        "id": "dCs5qhiS2PQZ",
        "outputId": "7eca2d88-dcd1-486a-d540-7b2cd3e780ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-27 05:20:29--  https://github.com/nik1806/Human-Pose-Detection/raw/refs/heads/master/model/coco/pose_iter_440000.caffemodel\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/nik1806/Human-Pose-Detection/refs/heads/master/model/coco/pose_iter_440000.caffemodel [following]\n",
            "--2025-02-27 05:20:29--  https://media.githubusercontent.com/media/nik1806/Human-Pose-Detection/refs/heads/master/model/coco/pose_iter_440000.caffemodel\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 209274056 (200M) [application/octet-stream]\n",
            "Saving to: ‘pose/pose_iter_440000.caffemodel’\n",
            "\n",
            "pose/pose_iter_4400 100%[===================>] 199.58M   312MB/s    in 0.6s    \n",
            "\n",
            "2025-02-27 05:20:34 (312 MB/s) - ‘pose/pose_iter_440000.caffemodel’ saved [209274056/209274056]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gradio if needed\n",
        "!pip install gradio\n",
        "\n",
        "# Add this after all the setup cells (after model downloads and directory creation)\n",
        "import gradio as gr\n",
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import time\n",
        "import u2net_run  # Ensure U-2-Net is properly imported\n",
        "\n",
        "def process_images(human_img, cloth_img):\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Debug: Print current directory\n",
        "        print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "        # Create necessary directories\n",
        "        os.makedirs('inputs/img', exist_ok=True)\n",
        "        os.makedirs('inputs/cloth', exist_ok=True)\n",
        "        os.makedirs('Data_preprocessing/test_img', exist_ok=True)\n",
        "        os.makedirs('Data_preprocessing/test_color', exist_ok=True)\n",
        "        os.makedirs('Data_preprocessing/test_edge', exist_ok=True)\n",
        "\n",
        "        # Fixed file paths\n",
        "        human_path = 'inputs/img/000001_0.png'\n",
        "        cloth_path = 'inputs/cloth/000001_1.png'\n",
        "        test_img_path = 'Data_preprocessing/test_img/000001_0.png'\n",
        "        test_color_path = 'Data_preprocessing/test_color/000001_1.png'\n",
        "        test_edge_path = 'Data_preprocessing/test_edge/000001_1.png'\n",
        "\n",
        "        # Convert and save images\n",
        "        if human_img is not None:\n",
        "            img = Image.open(human_img)\n",
        "            img = img.resize((192,256), Image.BICUBIC)\n",
        "            img.save(human_path)\n",
        "            img.save(test_img_path)\n",
        "            print(\"Human image saved successfully\")\n",
        "\n",
        "        if cloth_img is not None:\n",
        "            img = Image.open(cloth_img)\n",
        "            img = img.resize((192,256), Image.BICUBIC).convert('RGB')\n",
        "            img.save(cloth_path)\n",
        "            img.save(test_color_path)\n",
        "            print(\"Cloth image saved successfully\")\n",
        "\n",
        "        # Generate edge map using U-2-Net\n",
        "        print(\"Generating edge map...\")\n",
        "        u2net_run.infer(u2net, 'Data_preprocessing/test_color', 'Data_preprocessing/test_edge')\n",
        "\n",
        "        # Remove previous results\n",
        "        shutil.rmtree('results', ignore_errors=True)\n",
        "        print(\"Cleaned previous results\")\n",
        "\n",
        "        # Run segmentation\n",
        "        print(\"Running segmentation...\")\n",
        "        !python3 Self-Correction-Human-Parsing-for-ACGPN/simple_extractor.py \\\n",
        "            --dataset 'lip' \\\n",
        "            --model-restore 'lip_final.pth' \\\n",
        "            --input-dir 'Data_preprocessing/test_img' \\\n",
        "            --output-dir 'Data_preprocessing/test_label'\n",
        "\n",
        "        # Generate pose keypoints\n",
        "        print(\"Generating pose keypoints...\")\n",
        "        pose_path = 'Data_preprocessing/test_pose/000001_0_keypoints.json'\n",
        "        os.makedirs('Data_preprocessing/test_pose', exist_ok=True)\n",
        "        generate_pose_keypoints(test_img_path, pose_path)\n",
        "\n",
        "        # Create test pairs file\n",
        "        with open('Data_preprocessing/test_pairs.txt','w') as f:\n",
        "            f.write('000001_0.png 000001_1.png')\n",
        "        print(\"Created test pairs file\")\n",
        "\n",
        "        # Run inference\n",
        "        print(\"Running inference...\")\n",
        "        !python test.py\n",
        "\n",
        "        # Load and return result\n",
        "        result_path = 'results/test/try-on/000001_0.png'\n",
        "        if os.path.exists(result_path):\n",
        "            print(\"Processing completed successfully!\")\n",
        "            print(f\"Total time: {time.time() - start_time:.2f} seconds\")\n",
        "            return result_path\n",
        "        else:\n",
        "            return \"Error: No output generated. Check console for details.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_msg = f\"Error during processing:\\n{str(e)}\\n\\nTraceback:\\n{traceback.format_exc()}\"\n",
        "        print(error_msg)\n",
        "        return error_msg\n",
        "\n",
        "# Create Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=process_images,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"filepath\", label=\"Upload Human Image\"),\n",
        "        gr.Image(type=\"filepath\", label=\"Upload Cloth Image\")\n",
        "    ],\n",
        "    outputs=gr.Image(label=\"Result\"),\n",
        "    title=\"Virtual Try-On Demo\",\n",
        "    description=\"Upload a human image and a clothing image to see the virtual try-on result\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch(debug=True)"
      ],
      "metadata": {
        "id": "xpuDxDHp1heQ",
        "outputId": "6158262d-6ab8-4c0d-bd08-45c06c025213",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.19.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.8)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.7.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.7.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.9.7)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.45.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://3a34e22d6742bc8403.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3a34e22d6742bc8403.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content/ACGPN\n",
            "Human image saved successfully\n",
            "Cloth image saved successfully\n",
            "Generating edge map...\n",
            "Generating mask for: 000001_1.png\n",
            "Saving output at Data_preprocessing/test_edge/000001_1.png\n",
            "Cleaned previous results\n",
            "Running segmentation...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/content/ACGPN/Self-Correction-Human-Parsing-for-ACGPN/simple_extractor.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(args.model_restore)['state_dict']\n",
            "100% 1/1 [00:00<00:00,  2.74it/s]\n",
            "Generating pose keypoints...\n",
            "File saved at Data_preprocessing/test_pose/000001_0_keypoints.json\n",
            "Created test pairs file\n",
            "Running inference...\n",
            "?\n",
            "------------ Options -------------\n",
            "aspect_ratio: 1.0\n",
            "batchSize: 1\n",
            "checkpoints_dir: ./checkpoints\n",
            "cluster_path: features_clustered_010.npy\n",
            "continue_train: False\n",
            "data_type: 32\n",
            "datapairs: test_pairs.txt\n",
            "dataroot: Data_preprocessing/\n",
            "display_winsize: 512\n",
            "engine: None\n",
            "export_onnx: None\n",
            "fineSize: 512\n",
            "gpu_ids: [0]\n",
            "how_many: 1000\n",
            "input_nc: 3\n",
            "isTrain: False\n",
            "label_nc: 20\n",
            "loadSize: 512\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 1\n",
            "n_blocks_global: 4\n",
            "n_blocks_local: 3\n",
            "n_downsample_global: 4\n",
            "n_local_enhancers: 1\n",
            "name: label2city\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "norm: instance\n",
            "ntest: inf\n",
            "onnx: None\n",
            "output_nc: 3\n",
            "phase: test\n",
            "resize_or_crop: scale_width\n",
            "results_dir: ./results/\n",
            "serial_batches: True\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "use_encoded_image: False\n",
            "verbose: False\n",
            "which_epoch: latest\n",
            "-------------- End ----------------\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "Data_preprocessing/test_label label\n",
            "Data_preprocessing/test_img img\n",
            "Data_preprocessing/test_edge edge\n",
            "Data_preprocessing/test_edge edge\n",
            "Data_preprocessing/test_mask mask\n",
            "Data_preprocessing/test_mask mask\n",
            "Data_preprocessing/test_colormask colormask\n",
            "Data_preprocessing/test_colormask colormask\n",
            "Data_preprocessing/test_color color\n",
            "Data_preprocessing/test_color color\n",
            "# Inference images = 1\n",
            "latest_net_U.pth\n",
            "/content/ACGPN/models/base_model.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  network.load_state_dict(torch.load(save_path))\n",
            "latest_net_G1.pth\n",
            "latest_net_G2.pth\n",
            "latest_net_G.pth\n",
            "/content/ACGPN/models/pix2pixHD_model.py:227: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
            "  input_label = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/instancenorm.py:115: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:4902: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "Saving 000001_0.png\n",
            "Processing completed successfully!\n",
            "Total time: 19.66 seconds\n",
            "Current working directory: /content/ACGPN\n",
            "Human image saved successfully\n",
            "Cloth image saved successfully\n",
            "Generating edge map...\n",
            "Generating mask for: 000001_1.png\n",
            "Saving output at Data_preprocessing/test_edge/000001_1.png\n",
            "Cleaned previous results\n",
            "Running segmentation...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/content/ACGPN/Self-Correction-Human-Parsing-for-ACGPN/simple_extractor.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(args.model_restore)['state_dict']\n",
            "100% 1/1 [00:00<00:00,  2.72it/s]\n",
            "Generating pose keypoints...\n",
            "File saved at Data_preprocessing/test_pose/000001_0_keypoints.json\n",
            "Created test pairs file\n",
            "Running inference...\n",
            "?\n",
            "------------ Options -------------\n",
            "aspect_ratio: 1.0\n",
            "batchSize: 1\n",
            "checkpoints_dir: ./checkpoints\n",
            "cluster_path: features_clustered_010.npy\n",
            "continue_train: False\n",
            "data_type: 32\n",
            "datapairs: test_pairs.txt\n",
            "dataroot: Data_preprocessing/\n",
            "display_winsize: 512\n",
            "engine: None\n",
            "export_onnx: None\n",
            "fineSize: 512\n",
            "gpu_ids: [0]\n",
            "how_many: 1000\n",
            "input_nc: 3\n",
            "isTrain: False\n",
            "label_nc: 20\n",
            "loadSize: 512\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 1\n",
            "n_blocks_global: 4\n",
            "n_blocks_local: 3\n",
            "n_downsample_global: 4\n",
            "n_local_enhancers: 1\n",
            "name: label2city\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "norm: instance\n",
            "ntest: inf\n",
            "onnx: None\n",
            "output_nc: 3\n",
            "phase: test\n",
            "resize_or_crop: scale_width\n",
            "results_dir: ./results/\n",
            "serial_batches: True\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "use_encoded_image: False\n",
            "verbose: False\n",
            "which_epoch: latest\n",
            "-------------- End ----------------\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "Data_preprocessing/test_label label\n",
            "Data_preprocessing/test_img img\n",
            "Data_preprocessing/test_edge edge\n",
            "Data_preprocessing/test_edge edge\n",
            "Data_preprocessing/test_mask mask\n",
            "Data_preprocessing/test_mask mask\n",
            "Data_preprocessing/test_colormask colormask\n",
            "Data_preprocessing/test_colormask colormask\n",
            "Data_preprocessing/test_color color\n",
            "Data_preprocessing/test_color color\n",
            "# Inference images = 1\n",
            "latest_net_U.pth\n",
            "/content/ACGPN/models/base_model.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  network.load_state_dict(torch.load(save_path))\n",
            "latest_net_G1.pth\n",
            "latest_net_G2.pth\n",
            "latest_net_G.pth\n",
            "/content/ACGPN/models/pix2pixHD_model.py:227: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
            "  input_label = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/instancenorm.py:115: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:4902: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "Saving 000001_0.png\n",
            "Processing completed successfully!\n",
            "Total time: 18.90 seconds\n",
            "Current working directory: /content/ACGPN\n",
            "Human image saved successfully\n",
            "Cloth image saved successfully\n",
            "Generating edge map...\n",
            "Generating mask for: 000001_1.png\n",
            "Saving output at Data_preprocessing/test_edge/000001_1.png\n",
            "Cleaned previous results\n",
            "Running segmentation...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/content/ACGPN/Self-Correction-Human-Parsing-for-ACGPN/simple_extractor.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(args.model_restore)['state_dict']\n",
            "100% 1/1 [00:00<00:00,  1.87it/s]\n",
            "Generating pose keypoints...\n",
            "File saved at Data_preprocessing/test_pose/000001_0_keypoints.json\n",
            "Created test pairs file\n",
            "Running inference...\n",
            "?\n",
            "------------ Options -------------\n",
            "aspect_ratio: 1.0\n",
            "batchSize: 1\n",
            "checkpoints_dir: ./checkpoints\n",
            "cluster_path: features_clustered_010.npy\n",
            "continue_train: False\n",
            "data_type: 32\n",
            "datapairs: test_pairs.txt\n",
            "dataroot: Data_preprocessing/\n",
            "display_winsize: 512\n",
            "engine: None\n",
            "export_onnx: None\n",
            "fineSize: 512\n",
            "gpu_ids: [0]\n",
            "how_many: 1000\n",
            "input_nc: 3\n",
            "isTrain: False\n",
            "label_nc: 20\n",
            "loadSize: 512\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 1\n",
            "n_blocks_global: 4\n",
            "n_blocks_local: 3\n",
            "n_downsample_global: 4\n",
            "n_local_enhancers: 1\n",
            "name: label2city\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "norm: instance\n",
            "ntest: inf\n",
            "onnx: None\n",
            "output_nc: 3\n",
            "phase: test\n",
            "resize_or_crop: scale_width\n",
            "results_dir: ./results/\n",
            "serial_batches: True\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "use_encoded_image: False\n",
            "verbose: False\n",
            "which_epoch: latest\n",
            "-------------- End ----------------\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "Data_preprocessing/test_label label\n",
            "Data_preprocessing/test_img img\n",
            "Data_preprocessing/test_edge edge\n",
            "Data_preprocessing/test_edge edge\n",
            "Data_preprocessing/test_mask mask\n",
            "Data_preprocessing/test_mask mask\n",
            "Data_preprocessing/test_colormask colormask\n",
            "Data_preprocessing/test_colormask colormask\n",
            "Data_preprocessing/test_color color\n",
            "Data_preprocessing/test_color color\n",
            "# Inference images = 1\n",
            "latest_net_U.pth\n",
            "/content/ACGPN/models/base_model.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  network.load_state_dict(torch.load(save_path))\n",
            "latest_net_G1.pth\n",
            "latest_net_G2.pth\n",
            "latest_net_G.pth\n",
            "/content/ACGPN/models/pix2pixHD_model.py:227: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
            "  input_label = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/instancenorm.py:115: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:4902: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "Saving 000001_0.png\n",
            "Processing completed successfully!\n",
            "Total time: 18.90 seconds\n",
            "Current working directory: /content/ACGPN\n",
            "Human image saved successfully\n",
            "Cloth image saved successfully\n",
            "Generating edge map...\n",
            "Generating mask for: 000001_1.png\n",
            "Saving output at Data_preprocessing/test_edge/000001_1.png\n",
            "Cleaned previous results\n",
            "Running segmentation...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/content/ACGPN/Self-Correction-Human-Parsing-for-ACGPN/simple_extractor.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(args.model_restore)['state_dict']\n",
            "100% 1/1 [00:00<00:00,  2.66it/s]\n",
            "Generating pose keypoints...\n",
            "File saved at Data_preprocessing/test_pose/000001_0_keypoints.json\n",
            "Created test pairs file\n",
            "Running inference...\n",
            "?\n",
            "------------ Options -------------\n",
            "aspect_ratio: 1.0\n",
            "batchSize: 1\n",
            "checkpoints_dir: ./checkpoints\n",
            "cluster_path: features_clustered_010.npy\n",
            "continue_train: False\n",
            "data_type: 32\n",
            "datapairs: test_pairs.txt\n",
            "dataroot: Data_preprocessing/\n",
            "display_winsize: 512\n",
            "engine: None\n",
            "export_onnx: None\n",
            "fineSize: 512\n",
            "gpu_ids: [0]\n",
            "how_many: 1000\n",
            "input_nc: 3\n",
            "isTrain: False\n",
            "label_nc: 20\n",
            "loadSize: 512\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 1\n",
            "n_blocks_global: 4\n",
            "n_blocks_local: 3\n",
            "n_downsample_global: 4\n",
            "n_local_enhancers: 1\n",
            "name: label2city\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "norm: instance\n",
            "ntest: inf\n",
            "onnx: None\n",
            "output_nc: 3\n",
            "phase: test\n",
            "resize_or_crop: scale_width\n",
            "results_dir: ./results/\n",
            "serial_batches: True\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "use_encoded_image: False\n",
            "verbose: False\n",
            "which_epoch: latest\n",
            "-------------- End ----------------\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "Data_preprocessing/test_label label\n",
            "Data_preprocessing/test_img img\n",
            "Data_preprocessing/test_edge edge\n",
            "Data_preprocessing/test_edge edge\n",
            "Data_preprocessing/test_mask mask\n",
            "Data_preprocessing/test_mask mask\n",
            "Data_preprocessing/test_colormask colormask\n",
            "Data_preprocessing/test_colormask colormask\n",
            "Data_preprocessing/test_color color\n",
            "Data_preprocessing/test_color color\n",
            "# Inference images = 1\n",
            "latest_net_U.pth\n",
            "/content/ACGPN/models/base_model.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  network.load_state_dict(torch.load(save_path))\n",
            "latest_net_G1.pth\n",
            "latest_net_G2.pth\n",
            "latest_net_G.pth\n",
            "/content/ACGPN/models/pix2pixHD_model.py:227: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
            "  input_label = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/instancenorm.py:115: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:4902: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "Saving 000001_0.png\n",
            "Processing completed successfully!\n",
            "Total time: 19.05 seconds\n",
            "Current working directory: /content/ACGPN\n",
            "Human image saved successfully\n",
            "Cloth image saved successfully\n",
            "Generating edge map...\n",
            "Generating mask for: 000001_1.png\n",
            "Saving output at Data_preprocessing/test_edge/000001_1.png\n",
            "Cleaned previous results\n",
            "Running segmentation...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/content/ACGPN/Self-Correction-Human-Parsing-for-ACGPN/simple_extractor.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(args.model_restore)['state_dict']\n",
            "100% 1/1 [00:00<00:00,  2.73it/s]\n",
            "Generating pose keypoints...\n",
            "File saved at Data_preprocessing/test_pose/000001_0_keypoints.json\n",
            "Created test pairs file\n",
            "Running inference...\n",
            "?\n",
            "------------ Options -------------\n",
            "aspect_ratio: 1.0\n",
            "batchSize: 1\n",
            "checkpoints_dir: ./checkpoints\n",
            "cluster_path: features_clustered_010.npy\n",
            "continue_train: False\n",
            "data_type: 32\n",
            "datapairs: test_pairs.txt\n",
            "dataroot: Data_preprocessing/\n",
            "display_winsize: 512\n",
            "engine: None\n",
            "export_onnx: None\n",
            "fineSize: 512\n",
            "gpu_ids: [0]\n",
            "how_many: 1000\n",
            "input_nc: 3\n",
            "isTrain: False\n",
            "label_nc: 20\n",
            "loadSize: 512\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 1\n",
            "n_blocks_global: 4\n",
            "n_blocks_local: 3\n",
            "n_downsample_global: 4\n",
            "n_local_enhancers: 1\n",
            "name: label2city\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "norm: instance\n",
            "ntest: inf\n",
            "onnx: None\n",
            "output_nc: 3\n",
            "phase: test\n",
            "resize_or_crop: scale_width\n",
            "results_dir: ./results/\n",
            "serial_batches: True\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "use_encoded_image: False\n",
            "verbose: False\n",
            "which_epoch: latest\n",
            "-------------- End ----------------\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "Data_preprocessing/test_label label\n",
            "Data_preprocessing/test_img img\n",
            "Data_preprocessing/test_edge edge\n",
            "Data_preprocessing/test_edge edge\n",
            "Data_preprocessing/test_mask mask\n",
            "Data_preprocessing/test_mask mask\n",
            "Data_preprocessing/test_colormask colormask\n",
            "Data_preprocessing/test_colormask colormask\n",
            "Data_preprocessing/test_color color\n",
            "Data_preprocessing/test_color color\n",
            "# Inference images = 1\n",
            "latest_net_U.pth\n",
            "/content/ACGPN/models/base_model.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  network.load_state_dict(torch.load(save_path))\n",
            "latest_net_G1.pth\n",
            "latest_net_G2.pth\n",
            "latest_net_G.pth\n",
            "/content/ACGPN/models/pix2pixHD_model.py:227: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
            "  input_label = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/instancenorm.py:115: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:4902: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "Saving 000001_0.png\n",
            "Processing completed successfully!\n",
            "Total time: 18.62 seconds\n",
            "Current working directory: /content/ACGPN\n",
            "Human image saved successfully\n",
            "Cloth image saved successfully\n",
            "Generating edge map...\n",
            "Generating mask for: 000001_1.png\n",
            "Saving output at Data_preprocessing/test_edge/000001_1.png\n",
            "Cleaned previous results\n",
            "Running segmentation...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/content/ACGPN/Self-Correction-Human-Parsing-for-ACGPN/simple_extractor.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(args.model_restore)['state_dict']\n",
            "100% 1/1 [00:00<00:00,  2.68it/s]\n",
            "Generating pose keypoints...\n",
            "File saved at Data_preprocessing/test_pose/000001_0_keypoints.json\n",
            "Created test pairs file\n",
            "Running inference...\n",
            "?\n",
            "------------ Options -------------\n",
            "aspect_ratio: 1.0\n",
            "batchSize: 1\n",
            "checkpoints_dir: ./checkpoints\n",
            "cluster_path: features_clustered_010.npy\n",
            "continue_train: False\n",
            "data_type: 32\n",
            "datapairs: test_pairs.txt\n",
            "dataroot: Data_preprocessing/\n",
            "display_winsize: 512\n",
            "engine: None\n",
            "export_onnx: None\n",
            "fineSize: 512\n",
            "gpu_ids: [0]\n",
            "how_many: 1000\n",
            "input_nc: 3\n",
            "isTrain: False\n",
            "label_nc: 20\n",
            "loadSize: 512\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 1\n",
            "n_blocks_global: 4\n",
            "n_blocks_local: 3\n",
            "n_downsample_global: 4\n",
            "n_local_enhancers: 1\n",
            "name: label2city\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "norm: instance\n",
            "ntest: inf\n",
            "onnx: None\n",
            "output_nc: 3\n",
            "phase: test\n",
            "resize_or_crop: scale_width\n",
            "results_dir: ./results/\n",
            "serial_batches: True\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "use_encoded_image: False\n",
            "verbose: False\n",
            "which_epoch: latest\n",
            "-------------- End ----------------\n",
            "CustomDatasetDataLoader\n",
            "dataset [AlignedDataset] was created\n",
            "Data_preprocessing/test_label label\n",
            "Data_preprocessing/test_img img\n",
            "Data_preprocessing/test_edge edge\n",
            "Data_preprocessing/test_edge edge\n",
            "Data_preprocessing/test_mask mask\n",
            "Data_preprocessing/test_mask mask\n",
            "Data_preprocessing/test_colormask colormask\n",
            "Data_preprocessing/test_colormask colormask\n",
            "Data_preprocessing/test_color color\n",
            "Data_preprocessing/test_color color\n",
            "# Inference images = 1\n",
            "latest_net_U.pth\n",
            "/content/ACGPN/models/base_model.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  network.load_state_dict(torch.load(save_path))\n",
            "latest_net_G1.pth\n",
            "latest_net_G2.pth\n",
            "latest_net_G.pth\n",
            "/content/ACGPN/models/pix2pixHD_model.py:227: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
            "  input_label = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/instancenorm.py:115: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:4902: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "Saving 000001_0.png\n",
            "Processing completed successfully!\n",
            "Total time: 18.46 seconds\n"
          ]
        }
      ]
    }
  ]
}